Implement neural network in raw rust to get deeper understanding of it. 

## Example of back propagation

**Network Structure**:
- Input layer: \(x\)
- Hidden layer: \(z_1 = W_1 \cdot x + b_1\), \(a_1 = \sigma(z_1)\)
- Output layer: \(z_2 = W_2 \cdot a_1 + b_2\), \(a_2 = \sigma(z_2)\)
- Loss: \( \text{Loss} = -[y \log(a_2) + (1-y) \log(1-a_2)]\)

**Forward pass and Back propagation**:

| **Layer**        | **Forward Step**                                                                                           | **Backward Step**                                                                                                                                            |
|------------------|------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Input Layer**  | \( x \)                                                                                                    | N/A                                                                                                                                                          |
| **Hidden Layer** | **Forward**:                                                                                               | **Backward**:                                                                                                                                                |
|                  | \( z_1 = W_1 \cdot x + b_1 \)                                                                              | Compute \( \frac{\partial \text{Loss}}{\partial a_1} \):                                                                                                     |
|                  | \( a_1 = \sigma(z_1) \)                                                                                    | \(\frac{\partial \text{Loss}}{\partial a_1} = \left( \frac{\partial \text{Loss}}{\partial z_2} \right) \cdot \left( \frac{\partial z_2}{\partial a_1} \right)\) |
|                  |                                                                                                            | Compute \( \frac{\partial a_1}{\partial z_1} \):                                                                                                             |
|                  |                                                                                                            | \(\frac{\partial a_1}{\partial z_1} = a_1 (1 - a_1) \)                                                                                                       |
|                  |                                                                                                            | Compute \( \frac{\partial \text{Loss}}{\partial z_1} \):                                                                                                     |
|                  |                                                                                                            | \(\frac{\partial \text{Loss}}{\partial z_1} = \frac{\partial \text{Loss}}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1}\)                              |
|                  |                                                                                                            | Compute \( \frac{\partial z_1}{\partial W_1} \):                                                                                                             |
|                  |                                                                                                            | \(\frac{\partial z_1}{\partial W_1} = x \)                                                                                                                   |
|                  |                                                                                                            | Compute \( \frac{\partial \text{Loss}}{\partial W_1} \):                                                                                                     |
|                  |                                                                                                            | \(\frac{\partial \text{Loss}}{\partial W_1} = \frac{\partial \text{Loss}}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}\)                              |
|                  |                                                                                                            | **Update**: \( W_1 := W_1 - \eta \cdot \frac{\partial \text{Loss}}{\partial W_1} \)                                                                          |
| **Output Layer** | **Forward**:                                                                                               | **Backward**:                                                                                                                                                |
|                  | \( z_2 = W_2 \cdot a_1 + b_2 \)                                                                            | Compute \( \frac{\partial \text{Loss}}{\partial a_2} \):                                                                                                     |
|                  | \( a_2 = \sigma(z_2) \)                                                                                    | \(\frac{\partial \text{Loss}}{\partial a_2} = - \left( \frac{y}{a_2} - \frac{1-y}{1-a_2} \right) \)                                                          |
|                  |                                                                                                            | Compute \( \frac{\partial a_2}{\partial z_2} \):                                                                                                             |
|                  |                                                                                                            | \(\frac{\partial a_2}{\partial z_2} = a_2 (1 - a_2) \)                                                                                                       |
|                  |                                                                                                            | Compute \( \frac{\partial \text{Loss}}{\partial z_2} \):                                                                                                     |
|                  |                                                                                                            | \(\frac{\partial \text{Loss}}{\partial z_2} = \left( a_2 - y \right) \)                                                                                      |
|                  |                                                                                                            | Compute \( \frac{\partial z_2}{\partial W_2} \):                                                                                                             |
|                  |                                                                                                            | \(\frac{\partial z_2}{\partial W_2} = a_1 \)                                                                                                                 |
|                  |                                                                                                            | Compute \( \frac{\partial \text{Loss}}{\partial W_2} \):                                                                                                     |
|                  |                                                                                                            | \(\frac{\partial \text{Loss}}{\partial W_2} = (a_2 - y) \cdot a_1 \)                                                                                         |
|                  |                                                                                                            | **Update**: \( W_2 := W_2 - \eta \cdot \frac{\partial \text{Loss}}{\partial W_2} \)                                                                          |

This table summarizes the forward and backward steps for a simple neural network with one hidden layer and one output layer. The forward step involves computing the pre-activation and activation values for each layer, while the backward step involves calculating the gradients for each parameter and updating the weights accordingly.